{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T00:54:08.266012Z",
     "start_time": "2025-06-04T00:54:05.892497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This notebook is used to generate the mixed datasets\n",
    "import pandas as pd\n",
    "import ndjson\n",
    "import os"
   ],
   "id": "a8bdc6ce91e1b596",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T00:55:07.148639Z",
     "start_time": "2025-06-04T00:55:05.017053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# - reward-bench\n",
    "splits = {'raw': 'data/raw-00000-of-00001.parquet', 'filtered': 'data/filtered-00000-of-00001.parquet'}\n",
    "data_reward_bench = pd.read_parquet(\"hf://datasets/allenai/reward-bench/\" + splits[\"filtered\"])\n",
    "selected_data_reward_bench = data_reward_bench.loc[data_reward_bench['subset'].str.startswith(\"xstest-\") | data_reward_bench['subset'].str.startswith(\"refusals-\") | data_reward_bench['subset'].str.startswith(\"donotanswer\") | data_reward_bench['subset'].str.startswith(\"hep-\")\n",
    "]\n",
    "selected_data_reward_bench[\"new_id\"] = selected_data_reward_bench[\"subset\"] + \"/\" + selected_data_reward_bench[\"id\"].astype(str)\n",
    "selected_data_reward_bench = selected_data_reward_bench[['new_id', 'prompt']].rename(columns={'new_id': 'id'})"
   ],
   "id": "5baec5b02ae0f695",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dalton386/Desktop/LLMRouting/Codes/adaptive_llm_routing/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/p3/bpkqcww548192ylbj0g_z8th0000gn/T/ipykernel_37650/1427453898.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_data_reward_bench[\"new_id\"] = selected_data_reward_bench[\"subset\"] + \"/\" + selected_data_reward_bench[\"id\"].astype(str)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T00:55:14.521995Z",
     "start_time": "2025-06-04T00:55:11.671903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# - CodeUltraFeedback\n",
    "data_code_ultra_feedback = pd.read_parquet(\"hf://datasets/coseal/CodeUltraFeedback/data/train-00000-of-00001.parquet\")\n",
    "selected_data_code_ultra_feedback = data_code_ultra_feedback[['instruction']].reset_index()\n",
    "selected_data_code_ultra_feedback[\"id\"] = 'code_ultra_feedback/' + selected_data_code_ultra_feedback[\"index\"].astype(str)\n",
    "selected_data_code_ultra_feedback = selected_data_code_ultra_feedback[['id', 'instruction']].rename(columns={'instruction': 'prompt'})\n",
    "selected_data_code_ultra_feedback = selected_data_code_ultra_feedback.sample(n=1016, random_state=1)"
   ],
   "id": "2432358094e10486",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T00:55:48.452799Z",
     "start_time": "2025-06-04T00:55:15.353485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# - mix-instruct\n",
    "splits = {'train': 'train_data_prepared.jsonl', 'validation': 'val_data_prepared.jsonl', 'test': 'test_data_prepared.jsonl'}\n",
    "data_mix_instruct = pd.read_json(\"hf://datasets/llm-blender/mix-instruct/\" + splits[\"train\"], lines=True)\n",
    "selected_data_mix_instruct = data_mix_instruct[['id', 'instruction', 'input']]\n",
    "selected_data_mix_instruct['prompt'] = selected_data_mix_instruct['instruction'] + ' ' + selected_data_mix_instruct['input']\n",
    "selected_data_mix_instruct = selected_data_mix_instruct[['id', 'prompt']]\n",
    "selected_data_mix_instruct = selected_data_mix_instruct.sample(n=6000, random_state=1)"
   ],
   "id": "950084b989b64346",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/bpkqcww548192ylbj0g_z8th0000gn/T/ipykernel_37650/1389323287.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_data_mix_instruct['prompt'] = selected_data_mix_instruct['instruction'] + ' ' + selected_data_mix_instruct['input']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T00:55:52.099647Z",
     "start_time": "2025-06-04T00:55:51.346982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# - BeaverTails\n",
    "splits = {'330k_train': 'round0/330k/train.jsonl.xz', '330k_test': 'round0/330k/test.jsonl.xz', '30k_train': 'round0/30k/train.jsonl.gz', '30k_test': 'round0/30k/test.jsonl.gz'}\n",
    "data_beaver_tails = pd.read_json(\"hf://datasets/PKU-Alignment/BeaverTails/\" + splits[\"30k_train\"], lines=True)\n",
    "\n",
    "def is_prompt_harmful(category):\n",
    "    for k, v in category.items():\n",
    "        if v:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# select only the prompts that are harmful\n",
    "selected_data_beaver_tails = data_beaver_tails[data_beaver_tails['category'].apply(is_prompt_harmful)]\n",
    "selected_data_beaver_tails = selected_data_beaver_tails[['prompt']].drop_duplicates().reset_index()\n",
    "selected_data_beaver_tails[\"id\"] = 'beaver_tails/' + selected_data_beaver_tails[\"index\"].astype(str)\n",
    "selected_data_beaver_tails = selected_data_beaver_tails[['id', 'prompt']]\n",
    "selected_data_beaver_tails = selected_data_beaver_tails.sample(n=1260, random_state=1)"
   ],
   "id": "3074bbc882634da1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T00:57:20.525759Z",
     "start_time": "2025-06-04T00:57:20.459604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the selected data as a JSONL file\n",
    "selected_data = pd.concat([selected_data_reward_bench, selected_data_code_ultra_feedback, selected_data_beaver_tails, selected_data_mix_instruct])\n",
    "selected_data_dict = selected_data.to_dict(orient='records')\n",
    "\n",
    "file_path = '../data/mixed_dataset.jsonl'\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "with open(file_path, 'w', encoding='utf8') as f:\n",
    "    ndjson.dump(selected_data_dict, f, ensure_ascii=False)"
   ],
   "id": "71891fb26e2f1469",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# merge model responses\n",
    "from utils import load_sample\n",
    "import json\n",
    "\n",
    "model_name_list = ['gpt-4o', 'gpt-35-turbo', 'llama-31-8b', 'mistral-7b', 'mistral-8x7b', 'phi-3-medium', 'phi-3-mini']\n",
    "response_list = [load_sample(fname=f'../outputs/mixed_dataset_{model_name}.jsonl', is_jsonl=True) for model_name in model_name_list]\n",
    "\n",
    "data_original = response_list[0]\n",
    "for data_addon, model_addon in zip(response_list[1:], model_name_list[1:]):\n",
    "    for d1, d2 in zip(data_original, data_addon):\n",
    "        d1[model_addon] = d2[model_addon]\n",
    "\n",
    "file_name = '../data/mixed_dataset_ALL.jsonl'\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "with open(file_name, 'w') as f:\n",
    "    for d in data_original:\n",
    "        f.write(json.dumps(d) + \"\\n\")"
   ],
   "id": "4cd51dd5222b571f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T05:29:46.395581Z",
     "start_time": "2024-08-23T05:29:13.155577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split the mixed dataset into train, validation, and test sets\n",
    "from utils import load_sample\n",
    "import json\n",
    "\n",
    "# load and permutate the mixed dataset and split it into train, validation, and test sets\n",
    "data = load_sample(fname='../data/mixed_dataset_armoRM_ALL_token_num.jsonl')\n",
    "import random\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:int(0.8*len(data))]\n",
    "validation_data = data[int(0.8*len(data)):int(0.9*len(data))]\n",
    "test_data = data[int(0.9*len(data)):]\n",
    "print(len(train_data), len(validation_data), len(test_data))\n",
    "\n",
    "with open(f\"../data/mixed_dataset_armoRM_ALL_token_num_train.jsonl\", 'w') as f:\n",
    "    for d in train_data:\n",
    "        f.write(json.dumps(d) + \"\\n\")\n",
    "        \n",
    "with open(f\"../data/mixed_dataset_armoRM_ALL_token_num_validation.jsonl\", 'w') as f:\n",
    "    for d in validation_data:\n",
    "        f.write(json.dumps(d) + \"\\n\")\n",
    "        \n",
    "with open(f\"../data/mixed_dataset_armoRM_ALL_token_num_test.jsonl\", 'w') as f:\n",
    "    for d in test_data:\n",
    "        f.write(json.dumps(d) + \"\\n\")\n",
    "        "
   ],
   "id": "b79c16b468504a8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 1000 1000\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load the train splits of the mixed dataset and randomly select 10k response pairs stored as Dataset to train the distilled reward model.\n",
    "from utils import load_sample\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "model_name_list = ['gpt-4o', 'gpt-35-turbo', 'llama-31-8b', 'mistral-7b', 'mistral-8x7b', 'phi-3-medium', 'phi-3-mini']\n",
    "\n",
    "split_list = ['train', 'validation']\n",
    "split2response_dict = {}\n",
    "for split in split_list:\n",
    "    data = load_sample(fname=f'../data/mixed_dataset_armoRM_ALL_token_num_{split}.jsonl')\n",
    "    chosen_response_list = []\n",
    "    rejected_response_list = []\n",
    "    for d in data:\n",
    "        user_prompt = d['prompt']\n",
    "        for model_name in model_name_list:\n",
    "            model_response = d[model_name]['responses']\n",
    "            model_scores = d[model_name]['armoRM_scores']\n",
    "            if min(model_scores) == max(model_scores):\n",
    "                continue\n",
    "            sorted_model_response_by_score = sorted(zip(model_response, model_scores), key=lambda x: x[1])\n",
    "            chosen_response = sorted_model_response_by_score[-1][0]\n",
    "            rejected_response = sorted_model_response_by_score[0][0]\n",
    "            \n",
    "            # get the middle response\n",
    "            middle_idx = len(sorted_model_response_by_score)//2\n",
    "            middle_response = sorted_model_response_by_score[middle_idx][0]\n",
    "            \n",
    "            if min(model_scores) < sorted_model_response_by_score[middle_idx][1]:\n",
    "                chosen_response_list.append(f\"Human: {user_prompt} Assistant: {middle_response}\")\n",
    "                rejected_response_list.append(f\"Human: {user_prompt} Assistant: {rejected_response}\")\n",
    "            \n",
    "            if max(model_scores) > sorted_model_response_by_score[middle_idx][1]:\n",
    "                chosen_response_list.append(f\"Human: {user_prompt} Assistant: {chosen_response}\")\n",
    "                rejected_response_list.append(f\"Human: {user_prompt} Assistant: {middle_response}\")\n",
    "    \n",
    "    chosen_response_dataset = Dataset.from_dict({'chosen': chosen_response_list, 'rejected': rejected_response_list})\n",
    "    split2response_dict[split] = chosen_response_dataset\n",
    "    \n",
    "split2response_dataset_dict = DatasetDict({'train': split2response_dict['train'], 'validation': split2response_dict['validation']})\n",
    "split2response_dataset_dict.save_to_disk('../data/mixed_dataset_armoRM_ALL_token_num_reward_modelling_min_max_mid')"
   ],
   "id": "3c63645ed2ff00e2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
