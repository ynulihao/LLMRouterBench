{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "import sentencepiece\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "login('YOUR_HUGGINGFACE_TOKEN')\n",
    "\n",
    "tokenizer_gpt_4o = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "tokenizer_gpt_35_turbo = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "tokenizer_llama_31_8b = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "tokenizer_phi_3_mini = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "tokenizer_phi_3_medium = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-medium-128k-instruct\")\n",
    "tokenizer_mistral_7b = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer_mistral_8x7b = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "tokenizer_codestral_22b = AutoTokenizer.from_pretrained(\"mistralai/Codestral-22B-v0.1\")\n",
    "\n",
    "tokenizer_dict = {'gpt-4o': tokenizer_gpt_4o, 'gpt-35-turbo': tokenizer_gpt_35_turbo, 'llama-31-8b': tokenizer_llama_31_8b, 'phi-3-mini': tokenizer_phi_3_mini, 'phi-3-medium': tokenizer_phi_3_medium, 'mistral-7b': tokenizer_mistral_7b, 'mistral-8x7b': tokenizer_mistral_8x7b, 'codestral-22b': tokenizer_codestral_22b}"
   ],
   "id": "882c9035e3452506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from utils import load_sample\n",
    "data = load_sample('../data/mixed_dataset_armoRM_ALL.jsonl')\n",
    "model_name_list = ['gpt-4o', 'gpt-35-turbo', 'llama-31-8b', 'mistral-7b', 'mistral-8x7b', 'phi-3-medium', 'phi-3-mini']\n",
    "with open(f'../data/mixed_dataset_armoRM_ALL_token_num.jsonl', 'w') as f:\n",
    "    for i, d in enumerate(data):\n",
    "        for model_name in model_name_list:\n",
    "            d[model_name]['token_num_prompt'] = len(tokenizer_dict[model_name].encode(d['prompt']))\n",
    "            d[model_name]['token_num_responses'] = [len(tokenizer_dict[model_name].encode(_)) for _ in d[model_name]['responses']]\n",
    "        \n",
    "        f.write(json.dumps(d) + \"\\n\")"
   ],
   "id": "aaaaa3ce5ac842c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
